# -*- coding: utf-8 -*-
"""TCC_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MOK6ZzjLrPwJ9xSq2rzYUkoTfd84NIm-
"""

# Importando as bibliotecas #
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from io import open
pd.options.display.max_columns = 150

# Carregando Dataset # 
data = pd.read_csv('base_ml.csv', sep='\t')

# Verificando o Dataset#
data.head()

data["EXT1"].value_counts()

data.drop(data.columns[50:110], axis=1, inplace=True)

data.head()

data["EXT1"].value_counts()

pd.options.display.float_format = "{:.2f}".format
data.describe()

data[(data == 0.00).all(axis=1)].describe()

data = data[(data > 0.00).all(axis=1)]

data["EXT1"].value_counts()

!pip install yellowbrick

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer

# Instanciando o método Kmeans e o Visualizer #
kmeans = KMeans()
visualizer = KElbowVisualizer(kmeans, k=(2,10))

# Selecionando uma amostra aleatória dos dados com 5000 observações #
data_sample = data.sample(n=5000, random_state=1)

# Executando o teste para ver o número ideal de clusters #

visualizer.fit(data_sample)
visualizer.poof()

# Agrupando os participantes em 5 grupos - atribuindo registros aos devidos grupos #
kmeans =  KMeans(n_clusters=5)
k_fit = kmeans.fit(data)

# inserindo os rótulos dos clusters no dataframe #
predicoes = k_fit.labels_
data['Clusters'] = predicoes

# verificando os dados #
data.head()

data["Clusters"].value_counts()

# Agrupando os registros por grupos #
data.groupby('Clusters').mean()

# selecionando as colunas de cada Grupo # 
col_list = list(data)
ext = col_list[0:10]
est = col_list[10:20]
agr = col_list[20:30]
csn = col_list[30:40]
opn = col_list[40:50]

# Somando os valores de cada Grupo # 
data_soma = pd.DataFrame()
data_soma['extroversion'] = data[ext].sum(axis=1)/10
data_soma['neurotic'] = data[est].sum(axis=1)/10
data_soma['agreeable'] = data[agr].sum(axis=1)/10
data_soma['conscientious'] = data[csn].sum(axis=1)/10
data_soma['open'] = data[opn].sum(axis=1)/10
data_soma['clusters'] = predicoes

# exibindo o valor médio por grupo  #
 
data_soma.groupby('clusters').mean()

# Visualizando as médias por grupo #
data_clusters = data_soma.groupby('clusters').mean()

plt.figure(figsize=(22,3))
for i in range(0, 5):
    plt.subplot(1,5,i+1)
    plt.bar(data_clusters.columns, data_clusters.iloc[:, i], color='green', alpha=0.2)
    plt.plot(data_clusters.columns, data_clusters.iloc[:, i], color='red')
    plt.title('Grupo ' + str(i))
    plt.xticks(rotation=45)
    plt.ylim(0,4);

!pip install gradio

import gradio as gr

dicio_questions = open("questions.txt").read().split("\n")

dicio_questions

questions = []
for q in dicio_questions:
  q = str(q)
  questions.append(q[q.find("\t"):].lstrip())

questions

inputs_questions = []
for q in questions:
  obj_input = gr.inputs.Slider(minimum=1,maximum=5,step=1,default=3,label=q)
  inputs_questions.append(obj_input)

inputs_questions

def predict(*outputs_questions):
    outputs_questions = np.array(outputs_questions).reshape(1, -1)
    return k_fit.predict(outputs_questions)

iface = gr.Interface(
                    fn = predict,
                    title = "Big Five Personality",
                    description = "Sistema para detecção de traços de personalidade.",
                    inputs = inputs_questions,
                    outputs="text")
iface.launch(share=True)